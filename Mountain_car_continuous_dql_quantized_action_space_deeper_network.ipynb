{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Piras2024/QNL-Net-for-Image-Classification-Review-by-Matteo-Piras/blob/main/Mountain_car_continuous_dql_quantized_action_space_deeper_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCXLfbV5-ME5"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efe2157c"
      },
      "outputs": [],
      "source": [
        "# Define model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, num_actions, input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.FC = nn.Sequential(\n",
        "            nn.Linear(input_dim, 12),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(12, 8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(8, num_actions)\n",
        "            )\n",
        "\n",
        "        # Initialize FC layer weights using He initialization\n",
        "        for layer in [self.FC]:\n",
        "            for module in layer:\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.FC(x)\n",
        "        return Q\n",
        "\n",
        "# Define memory for Experience Replay\n",
        "class ReplayMemory():\n",
        "    def __init__(self, maxlen):\n",
        "        self.memory = deque([], maxlen=maxlen)\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, sample_size):\n",
        "        return random.sample(self.memory, sample_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30b0af39"
      },
      "outputs": [],
      "source": [
        "# MountainCar Deep Q-Learning\n",
        "class MountainCarDQL():\n",
        "\n",
        "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error\n",
        "    optimizer = None                # NN Optimizer. Initialize later.\n",
        "\n",
        "    def __init__(self, learning_rate_a=75e-5, discount_factor_g=0.96, network_sync_rate=100, replay_memory_size=100000, mini_batch_size=64, num_discrete_actions=10, seed=None, lr_decay_gamma=0.9, lr_step_size=1000, epsilon_decay_c1=1000, epsilon_decay_c2=1000, epsilon_decay_rate=0.0001, epsilon_min=0.01):\n",
        "        self.learning_rate_a = learning_rate_a\n",
        "        self.discount_factor_g = discount_factor_g\n",
        "        self.network_sync_rate = network_sync_rate\n",
        "        self.replay_memory_size = replay_memory_size\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.num_discrete_actions = num_discrete_actions\n",
        "        self.seed = seed\n",
        "        self.lr_decay_gamma = lr_decay_gamma # learning rate decay\n",
        "        self.lr_step_size = lr_step_size     # learning rate decay\n",
        "        self.epsilon_decay_c1 = epsilon_decay_c1 # hyperbolic epsilon decay\n",
        "        self.epsilon_decay_c2 = epsilon_decay_c2 # hyperbolic epsilon decay\n",
        "        self.epsilon_decay_rate = epsilon_decay_rate # exponential epsilon decay\n",
        "        self.epsilon_min = epsilon_min # minimum epsilon value\n",
        "\n",
        "        #To select which epsilon decay strategy is going to be used\n",
        "        self.linearDecay = True\n",
        "        self.hyperbolicDecay = False\n",
        "        self.exponentialDecay = False\n",
        "\n",
        "        #seed evrithing for reproducability\n",
        "        if self.seed is not None:\n",
        "            random.seed(self.seed)\n",
        "            np.random.seed(self.seed)\n",
        "            torch.manual_seed(self.seed)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed(self.seed)\n",
        "                torch.backends.cudnn.deterministic = True\n",
        "                torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "    # Train the environment\n",
        "    def train(self, episodes, render=False):\n",
        "        # Create MountainCarContinuous instance\n",
        "        env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
        "        # Wrap the environment with RecordVideo\n",
        "        env = gym.wrappers.RecordVideo(env, video_folder='mountaincar_train_video', episode_trigger=lambda x: x % 1000 == 0) # Record every 1000 episodes during training\n",
        "\n",
        "        # Set the seed for the environment\n",
        "        if self.seed is not None:\n",
        "            env.reset(seed=self.seed)\n",
        "\n",
        "\n",
        "        # Get continuous action space bounds\n",
        "        min_action = env.action_space.low[0]\n",
        "        max_action = env.action_space.high[0]\n",
        "\n",
        "        # Create discrete actions using linspace\n",
        "        self.discrete_actions = np.linspace(min_action, max_action, self.num_discrete_actions)\n",
        "\n",
        "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
        "        num_actions = self.num_discrete_actions # Use the number of discrete actions\n",
        "\n",
        "        epsilon = 1 # Initial epsilon\n",
        "\n",
        "        memory = ReplayMemory(self.replay_memory_size)\n",
        "\n",
        "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
        "        policy_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "        target_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "\n",
        "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
        "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "        # Policy network optimizer. \"Adam\" optimizer.\n",
        "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
        "\n",
        "        # Learning rate scheduler - for learning rate decay\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=self.lr_step_size, gamma=self.lr_decay_gamma)\n",
        "\n",
        "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
        "        rewards_per_episode = []\n",
        "\n",
        "        # List to keep track of epsilon decay\n",
        "        epsilon_history = []\n",
        "\n",
        "        # Track number of steps taken. Used for syncing policy => target network.\n",
        "        step_count=0\n",
        "        best_rewards=-200 # Adjusted initial best_rewards for continuous env\n",
        "        goal_reached=False #it is used to start training the network when the goal is reached at lest once in one of the episodes\n",
        "\n",
        "        for i in range(episodes):\n",
        "\n",
        "            state = env.reset()[0]  # Initialize to state 0\n",
        "            terminated = False      # True when agent reached goal\n",
        "            truncated = False\n",
        "\n",
        "            rewards = 0\n",
        "\n",
        "            # Agent navigates map until it falls into reaches goal (terminated), or the lenght of the episode is 999 (truncated).\n",
        "            while(not terminated and not truncated):\n",
        "\n",
        "                # Select action based on epsilon-greedy\n",
        "                if random.random() < epsilon:\n",
        "                    # select random action (index for discrete actions) uniformly\n",
        "                    action_index = random.randrange(self.num_discrete_actions)\n",
        "\n",
        "                else:\n",
        "                    # select best action (index for discrete actions)\n",
        "                    with torch.no_grad():\n",
        "                        # Use the continuous state as input and get the index of the best discrete action\n",
        "                        action_index = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
        "\n",
        "                # Map the discrete action index to the continuous action value\n",
        "                action = self.discrete_actions[action_index]\n",
        "\n",
        "                # Execute action - MountainCarContinuous expects a single value action in a list\n",
        "                new_state,reward,terminated,truncated,_ = env.step([action])\n",
        "\n",
        "                # Add a small negative reward at each timestep to discourage staying in the valley\n",
        "                reward -= 1\n",
        "\n",
        "                # Accumulate reward\n",
        "                rewards += reward\n",
        "\n",
        "                # Save experience into memory\n",
        "                memory.append((state, action_index, new_state, reward, terminated)) # Store action_index, not continuous action value\n",
        "\n",
        "                # Move to the next state\n",
        "                state = new_state\n",
        "\n",
        "                # Increment step counter\n",
        "                step_count+=1\n",
        "\n",
        "            # Keep track of the rewards collected per episode.\n",
        "            rewards_per_episode.append(rewards)\n",
        "\n",
        "            # Log reward per episode to wandb\n",
        "            wandb.log({\"reward_per_episode\": rewards}, step=i)\n",
        "\n",
        "\n",
        "            # Check if goal was reached\n",
        "            if(terminated):\n",
        "                goal_reached = True\n",
        "\n",
        "            # Graph training progress\n",
        "            #if(i!=0 and i%1000==0):\n",
        "                #print(f'Episode {i} Epsilon {epsilon}')\n",
        "\n",
        "                #self.plot_progress(rewards_per_episode, epsilon_history)\n",
        "                #torch.save(policy_dqn.state_dict(), f\"mountaincar_autosave_dql_{i}.pt\")\n",
        "\n",
        "\n",
        "            if rewards>best_rewards:\n",
        "                best_rewards = rewards\n",
        "                print(f'Best rewards so far: {best_rewards}')\n",
        "                # Save policy\n",
        "                torch.save(policy_dqn.state_dict(), f\"mountaincar_dql_{i}.pt\")\n",
        "\n",
        "\n",
        "            # Check if enough experience has been collected AND goal was reached\n",
        "            if len(memory)>self.mini_batch_size and goal_reached:\n",
        "                mini_batch = memory.sample(self.mini_batch_size) # Use mini_batch_size for sampling\n",
        "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
        "\n",
        "                #decay epsilon\n",
        "                if(self.linearDecay):\n",
        "                  epsilon = max(epsilon - 1/episodes, self.epsilon_min)\n",
        "                elif(self.hyperbolicDecay):\n",
        "                  epsilon = max(self.epsilon_decay_c1 / (self.epsilon_decay_c2 + i), self.epsilon_min)\n",
        "                elif(self.exponentialDecay):\n",
        "                  epsilon = self.epsilon_min + (1 - self.epsilon_min) * math.exp(-self.epsilon_decay_rate * i)\n",
        "\n",
        "                epsilon_history.append(epsilon)\n",
        "                # Log epsilon to wandb\n",
        "                wandb.log({\"epsilon\": epsilon}, step=i)\n",
        "\n",
        "                # Step the learning rate scheduler - for learning rate decay\n",
        "                self.scheduler.step()\n",
        "                # Log current learning rate to wandb\n",
        "                wandb.log({\"learning_rate\": self.optimizer.param_groups[0]['lr']}, step=i)\n",
        "\n",
        "\n",
        "                # Copy policy network to target network after a certain number of steps\n",
        "                if step_count > self.network_sync_rate:\n",
        "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "                    step_count=0\n",
        "\n",
        "        # Save the final model\n",
        "        #torch.save(policy_dqn.state_dict(), \"mountaincar_dql_final.pt\")\n",
        "        #print(\"Final model saved as mountaincar_dql_final.pt\")\n",
        "\n",
        "        # Close environment\n",
        "        env.close()\n",
        "    #def plot_progress(self, rewards_per_episode, epsilon_history):\n",
        "        # Create new graph\n",
        "        #plt.figure(1)\n",
        "\n",
        "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
        "        # rewards_curve = np.zeros(len(rewards_per_episode))\n",
        "        # for x in range(len(rewards_per_episode)):\n",
        "            # rewards_curve[x] = np.min(rewards_per_episode[max(0, x-10):(x+1)])\n",
        "        #plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
        "        # plt.plot(sum_rewards)\n",
        "        #plt.plot(rewards_per_episode)\n",
        "\n",
        "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
        "        #plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
        "        #plt.plot(epsilon_history)\n",
        "\n",
        "        # Save plots\n",
        "        #plt.savefig('mountaincar_dql.png')\n",
        "    # Optimize policy network\n",
        "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
        "\n",
        "        current_q_list = []\n",
        "        target_q_list = []\n",
        "\n",
        "        for state, action_index, new_state, reward, terminated in mini_batch: # Use action_index\n",
        "\n",
        "            if terminated:\n",
        "                # Agent receive reward of 100 for reaching goal.\n",
        "                # When in a terminated state, target q value should be set to the reward.\n",
        "                target = torch.FloatTensor([reward])\n",
        "            else:\n",
        "                # Calculate target q value\n",
        "                with torch.no_grad():\n",
        "                    # Use the continuous state as input\n",
        "                    target = torch.FloatTensor(\n",
        "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state)).max()\n",
        "                    )\n",
        "\n",
        "            # Get the current set of Q values\n",
        "            # Use the continuous state as input\n",
        "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
        "            current_q_list.append(current_q)\n",
        "\n",
        "            # Get the target set of Q values\n",
        "            # Use the continuous state as input\n",
        "            target_q = target_dqn(self.state_to_dqn_input(state))\n",
        "            # Adjust the specific action (index) to the target that was just calculated\n",
        "            target_q[action_index] = target\n",
        "            target_q_list.append(target_q)\n",
        "\n",
        "        # Compute loss for the whole minibatch\n",
        "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    '''\n",
        "    Converts a state (position, velocity) to tensor representation for continuous observation space.\n",
        "    Example:\n",
        "    Input = (0.3, -0.03)\n",
        "    Return = tensor([0.3, -0.03])\n",
        "    '''\n",
        "    def state_to_dqn_input(self, state)->torch.Tensor:\n",
        "        # The state is already a NumPy array [position, velocity]\n",
        "        # Convert it directly to a PyTorch FloatTensor\n",
        "        return torch.FloatTensor(state)\n",
        "\n",
        "    # Run the environment with the learned policy\n",
        "    def test(self, episodes, model_filepath):\n",
        "\n",
        "\n",
        "        env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
        "        # Wrap the environment with RecordVideo\n",
        "        env = gym.wrappers.RecordVideo(env, video_folder='mountaincar_test_video', episode_trigger=lambda x: True) # Record every episode\n",
        "\n",
        "        # Set the seed for the environment\n",
        "        if self.seed is not None:\n",
        "            env.reset(seed=self.seed)\n",
        "\n",
        "        # Get continuous action space bounds\n",
        "        min_action = env.action_space.low[0]\n",
        "        max_action = env.action_space.high[0]\n",
        "\n",
        "        # Create discrete actions using linspace\n",
        "        self.discrete_actions = np.linspace(min_action, max_action, self.num_discrete_actions)\n",
        "\n",
        "\n",
        "        num_states = env.observation_space.shape[0]\n",
        "        num_actions = self.num_discrete_actions # Use the number of discrete actions\n",
        "\n",
        "\n",
        "        # Load learned policy\n",
        "        policy_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
        "        policy_dqn.eval()    # switch model to evaluation mode\n",
        "\n",
        "        total_test_rewards = 0\n",
        "        total_test_modified_reward = 0\n",
        "        test_rewards_list = []\n",
        "        for i in range(episodes):\n",
        "            state = env.reset()[0]  # Initialize to state 0\n",
        "            terminated = False      # True when agent reached goal\n",
        "            truncated = False\n",
        "            rewards = 0\n",
        "            modifiedRewards = 0 #same rewards as training\n",
        "\n",
        "            while(not terminated and not truncated):\n",
        "                # Select best action (index)\n",
        "                with torch.no_grad():\n",
        "                    # Use the continuous state as input and get the index of the best discrete action\n",
        "                    action_index = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
        "\n",
        "                # Map the discrete action index to the continuous action value\n",
        "                action = self.discrete_actions[action_index]\n",
        "\n",
        "                # Execute action - MountainCarContinuous expects a single value action in a list\n",
        "                state,reward,terminated,truncated,_ = env.step([action])\n",
        "                rewards += reward\n",
        "                modifiedRewards += reward - 1 #same reward shaping used in training\n",
        "\n",
        "            total_test_rewards += rewards\n",
        "            test_rewards_list.append(rewards)\n",
        "            total_test_modified_reward += modifiedRewards\n",
        "\n",
        "            # Check if the goal was reached (terminated without truncation)\n",
        "            # MountainCarContinuous-v0 terminates when the flag is reached\n",
        "            if terminated:\n",
        "                print(f\"Episode {i+1}: Goal Reached! Reward: {rewards}\")\n",
        "            elif truncated:\n",
        "                print(f\"Episode {i+1}: Episode truncated (did not reach goal). Reward: {rewards}\")\n",
        "            else: # This case should not happen in MountainCarContinuous if not truncated\n",
        "                 print(f\"Episode {i+1}: Episode terminated unexpectedly. Reward: {rewards}\")\n",
        "\n",
        "        # Calculate and Log average test reward to wandb\n",
        "        if episodes > 0:\n",
        "            avg_test_reward = total_test_rewards / episodes\n",
        "            wandb.log({\"average_test_reward\": avg_test_reward})\n",
        "            print(f\"Average test reward over {episodes} episodes: {avg_test_reward}\")\n",
        "            avg_test_modified_reward = total_test_modified_reward / episodes\n",
        "            wandb.log({\"average_test_modified_reward\": avg_test_modified_reward})\n",
        "            print(f\"Average test modified reward over {episodes} episodes: {avg_test_modified_reward}\")\n",
        "        else:\n",
        "            print(\"No test episodes run.\")\n",
        "\n",
        "        # Log test videos to wandb\n",
        "        # Assuming videos are saved in 'mountaincar_test_video' directory\n",
        "        # Wandb can log video files directly.\n",
        "        # We need to find the video files generated during this test run.\n",
        "        # The RecordVideo wrapper names videos based on the episode index.\n",
        "        video_files = glob.glob('mountaincar_test_video/rl-video-episode-*.mp4')\n",
        "        if video_files:\n",
        "            print(f\"Logging {len(video_files)} test videos to wandb.\")\n",
        "            for video_file in video_files:\n",
        "                wandb.log({\"test_video\": wandb.Video(video_file)})\n",
        "        else:\n",
        "            print(\"No test videos found to log.\")\n",
        "\n",
        "\n",
        "        env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3517fdf4",
        "outputId": "87c3a1d3-470b-4c20-9e2b-639db3bc6ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished deleting files.\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Pattern per i file da cancellare\n",
        "file_pattern = \"mountaincar_*.pt\"\n",
        "\n",
        "# Trova tutti i file che corrispondono al pattern\n",
        "files_to_delete = glob.glob(file_pattern)\n",
        "\n",
        "# Itera sui file trovati e cancellali\n",
        "for file_path in files_to_delete:\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted: {file_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "print(\"Finished deleting files.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwZV4d4eSMrM"
      },
      "source": [
        "# Quantizzazione spazio delle azioni in 3 azioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "S40qAyRVZ2pf",
        "outputId": "59352c0d-26d4-44d4-ea48-56ca6272c9cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "creating run (0.0s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250912_122041-i3frbzvl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/i3frbzvl' target=\"_blank\">MountainCar_DQL_Run_3_Actions</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/i3frbzvl' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/i3frbzvl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/i3frbzvl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f57e86f3e60>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "num_discrete_actions=3\n",
        "learning_rate_a=0.01\n",
        "discount_factor_g=0.997\n",
        "seed=2025\n",
        "network_sync_rate=100\n",
        "replay_memory_size=100000\n",
        "mini_batch_size=64\n",
        "lr_decay_gamma=0.9\n",
        "lr_step_size=500\n",
        "epsilon_decay_c1=1000\n",
        "epsilon_decay_c2=1000\n",
        "# Define hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate_a\": learning_rate_a,\n",
        "    \"discount_factor_g\": discount_factor_g,\n",
        "    \"network_sync_rate\": network_sync_rate,\n",
        "    \"replay_memory_size\": replay_memory_size,\n",
        "    \"mini_batch_size\": mini_batch_size,\n",
        "    \"num_discrete_actions\": num_discrete_actions,\n",
        "    \"seed\": seed,\n",
        "    \"lr_decay_gamma\": lr_decay_gamma,\n",
        "    \"lr_step_size\": lr_step_size,\n",
        "    \"epsilon_decay_c1\": epsilon_decay_c1,\n",
        "    \"epsilon_decay_c2\": epsilon_decay_c2\n",
        "\n",
        "}\n",
        "\n",
        "wandb.init(project=\"MountainCar DQL\", name=\"MountainCar_DQL_Run_3_Actions\", config=hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTds2tqvwwBX",
        "outputId": "d6ebbfcb-9105-4fb0-b75c-8f48a8c064fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best rewards so far: -106.89999999999947\n",
            "Best rewards so far: -99.69999999999945\n",
            "Best rewards so far: -75.59999999999971\n",
            "Best rewards so far: -74.99999999999972\n",
            "Best rewards so far: -60.69999999999965\n",
            "Best rewards so far: -29.799999999999756\n",
            "Best rewards so far: -16.599999999999824\n",
            "Best rewards so far: 18.100000000000037\n",
            "Best rewards so far: 19.100000000000023\n",
            "Best rewards so far: 22.40000000000002\n"
          ]
        }
      ],
      "source": [
        "mountaincar = MountainCarDQL(num_discrete_actions=num_discrete_actions, learning_rate_a=learning_rate_a, discount_factor_g=discount_factor_g, seed=seed, lr_step_size=lr_step_size)\n",
        "\n",
        "mountaincar.train(10000, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8147d03",
        "outputId": "1212c0ee-7009-48df-caaa-c81624caff83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using latest model file: mountaincar_dql_4297.pt\n",
            "Episode 1: Goal Reached! Reward: 90.60000000000002\n",
            "Episode 2: Goal Reached! Reward: 93.50000000000001\n",
            "Episode 3: Goal Reached! Reward: 93.50000000000001\n",
            "Episode 4: Goal Reached! Reward: 93.4\n",
            "Episode 5: Goal Reached! Reward: 90.80000000000003\n",
            "Episode 6: Goal Reached! Reward: 90.80000000000003\n",
            "Episode 7: Goal Reached! Reward: 93.50000000000001\n",
            "Episode 8: Goal Reached! Reward: 93.4\n",
            "Episode 9: Goal Reached! Reward: 90.80000000000003\n",
            "Episode 10: Goal Reached! Reward: 90.60000000000002\n",
            "Episode 11: Goal Reached! Reward: 90.70000000000002\n",
            "Episode 12: Goal Reached! Reward: 90.80000000000003\n",
            "Episode 13: Goal Reached! Reward: 90.80000000000003\n",
            "Episode 14: Goal Reached! Reward: 93.4\n",
            "Episode 15: Goal Reached! Reward: 90.60000000000002\n",
            "Episode 16: Goal Reached! Reward: 90.80000000000003\n",
            "Episode 17: Goal Reached! Reward: 93.4\n",
            "Episode 18: Goal Reached! Reward: 90.80000000000003\n",
            "Episode 19: Goal Reached! Reward: 90.70000000000002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20: Goal Reached! Reward: 90.70000000000002\n",
            "Average test reward over 20 episodes: 91.68000000000002\n",
            "Average test modified reward over 20 episodes: 8.48000000000009\n",
            "Logging 20 test videos to wandb.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        }
      ],
      "source": [
        "# Find the latest saved model file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('mountaincar_dql_*.pt')\n",
        "if list_of_files:\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"Using latest model file: {latest_file}\")\n",
        "else:\n",
        "    latest_file = None\n",
        "    print(\"No model files found. Cannot run test.\")\n",
        "\n",
        "if latest_file:\n",
        "    mountaincar.test(20, latest_file)\n",
        "else:\n",
        "    print(\"Test skipped due to missing model file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5jO8tARgZAU"
      },
      "source": [
        "# Quantizzazione dello spazio delle azioni in 11 azioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kK93DcuggKV",
        "outputId": "81991898-fb75-41cd-e69e-b83c8598f476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted: mountaincar_dql_1165.pt\n",
            "Deleted: mountaincar_dql_2656.pt\n",
            "Deleted: mountaincar_dql_3849.pt\n",
            "Deleted: mountaincar_dql_3672.pt\n",
            "Deleted: mountaincar_dql_2503.pt\n",
            "Deleted: mountaincar_dql_1164.pt\n",
            "Finished deleting files.\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Pattern per i file da cancellare\n",
        "file_pattern = \"mountaincar_*.pt\"\n",
        "\n",
        "# Trova tutti i file che corrispondono al pattern\n",
        "files_to_delete = glob.glob(file_pattern)\n",
        "\n",
        "# Itera sui file trovati e cancellali\n",
        "for file_path in files_to_delete:\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted: {file_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "print(\"Finished deleting files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "collapsed": true,
        "id": "8c8490b9",
        "outputId": "7ade40c5-72b7-4cf9-d1a3-c367b9b276dc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for wandb.init()..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250912_111955-x8q0demt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/x8q0demt' target=\"_blank\">MountainCar_DQL_Run_11_Actions</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/x8q0demt' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/x8q0demt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/x8q0demt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f57e8a23e60>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "num_discrete_actions=11\n",
        "learning_rate_a=0.01\n",
        "discount_factor_g=0.997\n",
        "seed=2025\n",
        "network_sync_rate=100\n",
        "replay_memory_size=100000\n",
        "mini_batch_size=64\n",
        "lr_decay_gamma=0.9\n",
        "lr_step_size=500\n",
        "epsilon_decay_c1=1000\n",
        "epsilon_decay_c2=1000\n",
        "# Define hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate_a\": learning_rate_a,\n",
        "    \"discount_factor_g\": discount_factor_g,\n",
        "    \"network_sync_rate\": network_sync_rate,\n",
        "    \"replay_memory_size\": replay_memory_size,\n",
        "    \"mini_batch_size\": mini_batch_size,\n",
        "    \"num_discrete_actions\": num_discrete_actions,\n",
        "    \"seed\": seed,\n",
        "    \"lr_decay_gamma\": lr_decay_gamma,\n",
        "    \"lr_step_size\": lr_step_size,\n",
        "    \"epsilon_decay_c1\": epsilon_decay_c1,\n",
        "    \"epsilon_decay_c2\": epsilon_decay_c2\n",
        "\n",
        "}\n",
        "\n",
        "wandb.init(project=\"MountainCar DQL\", name=\"MountainCar_DQL_Run_11_Actions\", config=hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIH6DcDHglbP",
        "outputId": "dffbd56e-6f4e-4703-c6dc-ab914372bf1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best rewards so far: -179.57200007247832\n",
            "Best rewards so far: -178.67200022268236\n",
            "Best rewards so far: -83.87200046586949\n",
            "Best rewards so far: -39.524000149726504\n"
          ]
        }
      ],
      "source": [
        "mountaincar = MountainCarDQL(num_discrete_actions=num_discrete_actions, learning_rate_a=learning_rate_a, discount_factor_g=discount_factor_g, seed=seed, lr_step_size=lr_step_size)\n",
        "\n",
        "mountaincar.train(10000, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ8o7UO1viLj",
        "outputId": "88130d94-065b-4e50-ef06-adcc20cd9228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using latest model file: mountaincar_dql_3894.pt\n",
            "Episode 1: Goal Reached! Reward: 89.83999992847444\n",
            "Episode 2: Goal Reached! Reward: 90.21999991416934\n",
            "Episode 3: Goal Reached! Reward: 90.81999991416933\n",
            "Episode 4: Goal Reached! Reward: 92.64799990844728\n",
            "Episode 5: Goal Reached! Reward: 89.507999956131\n",
            "Episode 6: Goal Reached! Reward: 89.84399993515017\n",
            "Episode 7: Goal Reached! Reward: 92.1839999113083\n",
            "Episode 8: Goal Reached! Reward: 87.69599989318851\n",
            "Episode 9: Goal Reached! Reward: 89.76399993991853\n",
            "Episode 10: Goal Reached! Reward: 89.83999992847444\n",
            "Episode 11: Goal Reached! Reward: 89.78399994468691\n",
            "Episode 12: Goal Reached! Reward: 89.78799995136264\n",
            "Episode 13: Goal Reached! Reward: 89.68799995136263\n",
            "Episode 14: Goal Reached! Reward: 92.64799990844728\n",
            "Episode 15: Goal Reached! Reward: 90.04399993515017\n",
            "Episode 16: Goal Reached! Reward: 89.32399995422367\n",
            "Episode 17: Goal Reached! Reward: 90.00399992561343\n",
            "Episode 18: Goal Reached! Reward: 89.76399993991853\n",
            "Episode 19: Goal Reached! Reward: 89.76799994659426\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20: Goal Reached! Reward: 89.76799994659426\n",
            "Average test reward over 20 episodes: 90.14719993166925\n",
            "Average test modified reward over 20 episodes: -35.95280006833048\n",
            "Logging 21 test videos to wandb.\n"
          ]
        }
      ],
      "source": [
        "# Find the latest saved model file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('mountaincar_dql_*.pt')\n",
        "if list_of_files:\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"Using latest model file: {latest_file}\")\n",
        "else:\n",
        "    latest_file = None\n",
        "    print(\"No model files found. Cannot run test.\")\n",
        "\n",
        "if latest_file:\n",
        "    mountaincar.test(20, latest_file)\n",
        "else:\n",
        "    print(\"Test skipped due to missing model file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQPfnUZZWd_Y"
      },
      "source": [
        "# Quantizzazione dello spazio delle azioni in 101 azioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmYjQxxXWhNQ",
        "outputId": "6973eda3-eaf6-416a-db06-5eaab90df1a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted: mountaincar_dql_2123.pt\n",
            "Deleted: mountaincar_dql_3894.pt\n",
            "Deleted: mountaincar_dql_1112.pt\n",
            "Deleted: mountaincar_dql_2634.pt\n",
            "Finished deleting files.\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Pattern per i file da cancellare\n",
        "file_pattern = \"mountaincar_*.pt\"\n",
        "\n",
        "# Trova tutti i file che corrispondono al pattern\n",
        "files_to_delete = glob.glob(file_pattern)\n",
        "\n",
        "# Itera sui file trovati e cancellali\n",
        "for file_path in files_to_delete:\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted: {file_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "print(\"Finished deleting files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "5YYmPBe3Wky8",
        "outputId": "062c8be2-33ea-4878-b298-b6163f855b70"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_test_modified_reward</td><td></td></tr><tr><td>average_test_reward</td><td></td></tr><tr><td>epsilon</td><td></td></tr><tr><td>learning_rate</td><td></td></tr><tr><td>reward_per_episode</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_test_modified_reward</td><td>-35.9528</td></tr><tr><td>average_test_reward</td><td>90.1472</td></tr><tr><td>epsilon</td><td>0.09092</td></tr><tr><td>learning_rate</td><td>0.00135</td></tr><tr><td>reward_per_episode</td><td>-1092.828</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">MountainCar_DQL_Run_11_Actions</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/x8q0demt' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/x8q0demt</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a><br>Synced 5 W&B file(s), 21 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250912_111955-x8q0demt/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "creating run (0.5s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250912_114610-7i9ve9fw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/7i9ve9fw' target=\"_blank\">MountainCar_DQL_Run_101_Actions</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/7i9ve9fw' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/7i9ve9fw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/7i9ve9fw?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f57e86f2e10>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "num_discrete_actions=101\n",
        "learning_rate_a=0.01\n",
        "discount_factor_g=0.998\n",
        "seed=2025\n",
        "network_sync_rate=100\n",
        "replay_memory_size=100000\n",
        "mini_batch_size=64\n",
        "lr_decay_gamma=0.9\n",
        "lr_step_size=1000\n",
        "epsilon_decay_c1=1500\n",
        "epsilon_decay_c2=1500\n",
        "# Define hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate_a\": learning_rate_a,\n",
        "    \"discount_factor_g\": discount_factor_g,\n",
        "    \"network_sync_rate\": network_sync_rate,\n",
        "    \"replay_memory_size\": replay_memory_size,\n",
        "    \"mini_batch_size\": mini_batch_size,\n",
        "    \"num_discrete_actions\": num_discrete_actions,\n",
        "    \"seed\": seed,\n",
        "    \"lr_decay_gamma\": lr_decay_gamma,\n",
        "    \"lr_step_size\": lr_step_size,\n",
        "    \"epsilon_decay_c1\": epsilon_decay_c1,\n",
        "    \"epsilon_decay_c2\": epsilon_decay_c2\n",
        "\n",
        "}\n",
        "\n",
        "wandb.init(project=\"MountainCar DQL\", name=\"MountainCar_DQL_Run_101_Actions\", config=hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVwiCE3oW2oW",
        "outputId": "bf892420-7c42-467b-9128-edb70a514eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best rewards so far: -188.60455913561765\n",
            "Best rewards so far: -138.66067928034755\n",
            "Best rewards so far: -100.33139986286118\n",
            "Best rewards so far: -94.73983976164786\n",
            "Best rewards so far: -71.3848395730592\n",
            "Best rewards so far: -65.04091960866933\n",
            "Best rewards so far: -37.09419998793615\n",
            "Best rewards so far: -19.733958981313492\n",
            "Best rewards so far: -16.75647950603512\n",
            "Best rewards so far: -15.956199475336348\n",
            "Best rewards so far: -10.504279509992855\n"
          ]
        }
      ],
      "source": [
        "mountaincar = MountainCarDQL(num_discrete_actions=num_discrete_actions,\n",
        "                             learning_rate_a=learning_rate_a,\n",
        "                             discount_factor_g=discount_factor_g,\n",
        "                             seed=seed,\n",
        "                             lr_step_size=lr_step_size,\n",
        "                             epsilon_decay_c1=epsilon_decay_c1,\n",
        "                             epsilon_decay_c2=epsilon_decay_c2)\n",
        "\n",
        "mountaincar.train(10000, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjXuXu32XFCm",
        "outputId": "34f491d2-eff6-423a-98c4-3cf582da7a4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using latest model file: mountaincar_dql_6395.pt\n",
            "Episode 1: Goal Reached! Reward: 94.54592048461912\n",
            "Episode 2: Goal Reached! Reward: 93.187840480957\n",
            "Episode 3: Goal Reached! Reward: 93.1238404885864\n",
            "Episode 4: Goal Reached! Reward: 93.02140049562452\n",
            "Episode 5: Goal Reached! Reward: 94.40504049106595\n",
            "Episode 6: Goal Reached! Reward: 94.4819204922485\n",
            "Episode 7: Goal Reached! Reward: 93.08540048799512\n",
            "Episode 8: Goal Reached! Reward: 93.20072048976895\n",
            "Episode 9: Goal Reached! Reward: 94.4819204922485\n",
            "Episode 10: Goal Reached! Reward: 94.54592048461912\n",
            "Episode 11: Goal Reached! Reward: 94.50748048402784\n",
            "Episode 12: Goal Reached! Reward: 94.44348049165723\n",
            "Episode 13: Goal Reached! Reward: 94.44348049165723\n",
            "Episode 14: Goal Reached! Reward: 93.02140049562452\n",
            "Episode 15: Goal Reached! Reward: 94.54592048461912\n",
            "Episode 16: Goal Reached! Reward: 94.40504049106595\n",
            "Episode 17: Goal Reached! Reward: 93.22648050739285\n",
            "Episode 18: Goal Reached! Reward: 94.4819204922485\n",
            "Episode 19: Goal Reached! Reward: 94.44348049165723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20: Goal Reached! Reward: 94.44348049165723\n",
            "Average test reward over 20 episodes: 94.00210449046705\n",
            "Average test modified reward over 20 episodes: -17.047895509533184\n",
            "Logging 21 test videos to wandb.\n"
          ]
        }
      ],
      "source": [
        "# Find the latest saved model file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('mountaincar_dql_*.pt')\n",
        "if list_of_files:\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"Using latest model file: {latest_file}\")\n",
        "else:\n",
        "    latest_file = None\n",
        "    print(\"No model files found. Cannot run test.\")\n",
        "\n",
        "if latest_file:\n",
        "    mountaincar.test(20, latest_file)\n",
        "else:\n",
        "    print(\"Test skipped due to missing model file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz12MjgxkcHD"
      },
      "source": [
        "# Quantizzazione dello spazio delle azioni in 1001 azioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0JYdHbfkfji",
        "outputId": "09b55971-9b8d-4859-a88f-13eac153cae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished deleting files.\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Pattern per i file da cancellare\n",
        "file_pattern = \"mountaincar_*.pt\"\n",
        "\n",
        "# Trova tutti i file che corrispondono al pattern\n",
        "files_to_delete = glob.glob(file_pattern)\n",
        "\n",
        "# Itera sui file trovati e cancellali\n",
        "for file_path in files_to_delete:\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted: {file_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "print(\"Finished deleting files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "YIBcovVAkt_P",
        "outputId": "63c3ab6e-4434-4efb-dae9-4fe9d49a3b93"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td></td></tr><tr><td>learning_rate</td><td></td></tr><tr><td>reward_per_episode</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>0.15117</td></tr><tr><td>learning_rate</td><td>0.00314</td></tr><tr><td>reward_per_episode</td><td>-1006.21328</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">MountainCar_DQL_Run_1001_Actions</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/sft703ie' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/sft703ie</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250918_073133-sft703ie/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "creating run (0.0s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250918_074421-yd4xzxd0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/yd4xzxd0' target=\"_blank\">MountainCar_DQL_Run_1001_Actions</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/yd4xzxd0' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/yd4xzxd0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/yd4xzxd0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x797a5cdba030>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import wandb\n",
        "num_discrete_actions=1001\n",
        "learning_rate_a=0.01\n",
        "discount_factor_g=0.996\n",
        "seed=2025\n",
        "network_sync_rate=100\n",
        "replay_memory_size=100000\n",
        "mini_batch_size=64\n",
        "lr_decay_gamma=0.95\n",
        "lr_step_size=500\n",
        "#epsilon_decay_c1=2000\n",
        "#epsilon_decay_c2=2000\n",
        "epsilon_decay_rate=0.00015\n",
        "epsilon_min=0.01\n",
        "# Define hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate_a\": learning_rate_a,\n",
        "    \"discount_factor_g\": discount_factor_g,\n",
        "    \"network_sync_rate\": network_sync_rate,\n",
        "    \"replay_memory_size\": replay_memory_size,\n",
        "    \"mini_batch_size\": mini_batch_size,\n",
        "    \"num_discrete_actions\": num_discrete_actions,\n",
        "    \"seed\": seed,\n",
        "    \"lr_decay_gamma\": lr_decay_gamma,\n",
        "    \"lr_step_size\": lr_step_size,\n",
        "    #\"epsilon_decay_c1\": epsilon_decay_c1,\n",
        "    #\"epsilon_decay_c2\": epsilon_decay_c2,\n",
        "    \"epsilon_decay_rate\": epsilon_decay_rate,\n",
        "    \"epsilon_min\": epsilon_min\n",
        "\n",
        "}\n",
        "\n",
        "wandb.init(project=\"MountainCar DQL\", name=\"MountainCar_DQL_Run_1001_Actions\", config=hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ar58hoMky92",
        "outputId": "1dd73c43-1b94-4cf2-effc-6a9b71d458d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:296: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/mountaincar_train_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best rewards so far: -144.6535580346796\n",
            "Best rewards so far: -120.46285996840568\n",
            "Best rewards so far: -89.26621450071028\n",
            "Best rewards so far: -84.02450131862682\n",
            "Best rewards so far: -76.74484501359498\n",
            "Best rewards so far: -41.590696474251104\n",
            "Best rewards so far: -28.87293881617731\n",
            "Best rewards so far: -26.693107596262166\n",
            "Best rewards so far: -7.807361172613582\n"
          ]
        }
      ],
      "source": [
        "mountaincar = MountainCarDQL(num_discrete_actions=num_discrete_actions,\n",
        "                             learning_rate_a=learning_rate_a,\n",
        "                             discount_factor_g=discount_factor_g,\n",
        "                             seed=seed,\n",
        "                             lr_step_size=lr_step_size,\n",
        "                             #epsilon_decay_c1=epsilon_decay_c1,\n",
        "                             #epsilon_decay_c2=epsilon_decay_c2,\n",
        "                             epsilon_decay_rate=epsilon_decay_rate,\n",
        "                             epsilon_min=epsilon_min\n",
        "                             )\n",
        "\n",
        "mountaincar.train(10000, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz2ksG0HqtvD",
        "outputId": "66bafc9c-e2da-42e2-f818-bef165f8e24a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using latest model file: mountaincar_dql_9504.pt\n",
            "Episode 1: Goal Reached! Reward: 86.58709481586745\n",
            "Episode 2: Goal Reached! Reward: 93.21152286854395\n",
            "Episode 3: Goal Reached! Reward: 93.21266527670282\n",
            "Episode 4: Goal Reached! Reward: 93.01287928854937\n",
            "Episode 5: Episode truncated (did not reach goal). Reward: -82.00151968002557\n",
            "Episode 6: Goal Reached! Reward: 86.6001073337561\n",
            "Episode 7: Goal Reached! Reward: 93.07173048098444\n",
            "Episode 8: Goal Reached! Reward: 93.25663483342585\n",
            "Episode 9: Goal Reached! Reward: 86.71916257544083\n",
            "Episode 10: Goal Reached! Reward: 86.44480716333447\n",
            "Episode 11: Goal Reached! Reward: 86.47372538655533\n",
            "Episode 12: Episode truncated (did not reach goal). Reward: -82.00151968002557\n",
            "Episode 13: Episode truncated (did not reach goal). Reward: -82.00151968002557\n",
            "Episode 14: Goal Reached! Reward: 93.01287928854937\n",
            "Episode 15: Goal Reached! Reward: 92.6212885493186\n",
            "Episode 16: Episode truncated (did not reach goal). Reward: -82.00151968002557\n",
            "Episode 17: Goal Reached! Reward: 92.97266789948571\n",
            "Episode 18: Goal Reached! Reward: 86.71916257544083\n",
            "Episode 19: Goal Reached! Reward: 86.41487419412026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20: Goal Reached! Reward: 86.41487419412026\n",
            "Average test reward over 20 episodes: 55.43699990020466\n",
            "Average test modified reward over 20 episodes: -266.513000099795\n",
            "Logging 20 test videos to wandb.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        }
      ],
      "source": [
        "# Find the latest saved model file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('mountaincar_dql_*.pt')\n",
        "if list_of_files:\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"Using latest model file: {latest_file}\")\n",
        "else:\n",
        "    latest_file = None\n",
        "    print(\"No model files found. Cannot run test.\")\n",
        "\n",
        "if latest_file:\n",
        "    mountaincar.test(20, latest_file)\n",
        "else:\n",
        "    print(\"Test skipped due to missing model file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a92aad1"
      },
      "source": [
        "# Ulteriori tentativi per la quantizzazione in 1001 azioni\n",
        "Come ulteriore tentativo per ottenere una buona policy per il caso di 1001 azioni ho provato a modificare la Rete aumentando la dimensione dell'hidden layer e inoltre ho provato a fare reward shaping aggiungendo un reward positivo ogni volta che l'agente raggiunge una nuova posizione a destra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeAR4IB2pUiX"
      },
      "outputs": [],
      "source": [
        "# Define model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, num_actions, input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.FC = nn.Sequential(\n",
        "            nn.Linear(input_dim, 100),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(100, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(500, num_actions)\n",
        "            )\n",
        "\n",
        "        # Initialize FC layer weights using He initialization\n",
        "        for layer in [self.FC]:\n",
        "            for module in layer:\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.FC(x)\n",
        "        return Q\n",
        "\n",
        "# Define memory for Experience Replay\n",
        "class ReplayMemory():\n",
        "    def __init__(self, maxlen):\n",
        "        self.memory = deque([], maxlen=maxlen)\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, sample_size):\n",
        "        return random.sample(self.memory, sample_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6232521f"
      },
      "outputs": [],
      "source": [
        "class MountainCarDQL():\n",
        "\n",
        "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error.\n",
        "    optimizer = None                # NN Optimizer. Initialize later.\n",
        "\n",
        "    def __init__(self, learning_rate_a=75e-5, discount_factor_g=0.96, network_sync_rate=100, replay_memory_size=100000, mini_batch_size=64, num_discrete_actions=10, seed=None, lr_decay_gamma=0.9, lr_step_size=1000, epsilon_decay_c1=1000, epsilon_decay_c2=1000, epsilon_decay_rate=0.0001, epsilon_min=0.01):\n",
        "        self.learning_rate_a = learning_rate_a\n",
        "        self.discount_factor_g = discount_factor_g\n",
        "        self.network_sync_rate = network_sync_rate\n",
        "        self.replay_memory_size = replay_memory_size\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.num_discrete_actions = num_discrete_actions\n",
        "        self.seed = seed\n",
        "        self.lr_decay_gamma = lr_decay_gamma # learning rate decay\n",
        "        self.lr_step_size = lr_step_size     # learning rate decay\n",
        "        self.epsilon_decay_c1 = epsilon_decay_c1 # hyperbolic epsilon decay\n",
        "        self.epsilon_decay_c2 = epsilon_decay_c2 # hyperbolic epsilon decay\n",
        "        self.epsilon_decay_rate = epsilon_decay_rate # exponential epsilon decay\n",
        "        self.epsilon_min = epsilon_min # minimum epsilon value\n",
        "\n",
        "        self.linearDecay = False\n",
        "        self.hyperbolicDecay = False\n",
        "        self.exponentialDecay = True\n",
        "\n",
        "        if self.seed is not None:\n",
        "            random.seed(self.seed)\n",
        "            np.random.seed(self.seed)\n",
        "            torch.manual_seed(self.seed)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed(self.seed)\n",
        "                torch.backends.cudnn.deterministic = True\n",
        "                torch.backends.cudnn.benchmark = False\n",
        "\n",
        "        # Initialize max_position to the minimum possible position\n",
        "        # MountainCarContinuous-v0 has observation_space.low[0] as the minimum position\n",
        "        # Initialize it here, but the actual environment needs to be created first to get the low value.\n",
        "        # Let's initialize to None and get the value from the env inside train\n",
        "        self.max_position = None\n",
        "\n",
        "\n",
        "    # Train the environment\n",
        "    def train(self, episodes, render=False):\n",
        "        # Create MountainCarContinuous instance\n",
        "        env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
        "        # Wrap the environment with RecordVideo\n",
        "        env = gym.wrappers.RecordVideo(env, video_folder='mountaincar_train_video', episode_trigger=lambda x: x % 1000 == 0) # Record every 1000 episodes during training\n",
        "\n",
        "        # Set the seed for the environment\n",
        "        if self.seed is not None:\n",
        "            env.reset(seed=self.seed)\n",
        "\n",
        "        # Get continuous action space bounds\n",
        "        min_action = env.action_space.low[0]\n",
        "        max_action = env.action_space.high[0]\n",
        "\n",
        "        # Initialize max_position if it's None (first time training)\n",
        "        if self.max_position is None:\n",
        "             self.max_position = env.observation_space.low[0]\n",
        "\n",
        "\n",
        "        # Create discrete actions using linspace\n",
        "        self.discrete_actions = np.linspace(min_action, max_action, self.num_discrete_actions)\n",
        "\n",
        "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
        "        num_actions = self.num_discrete_actions # Use the number of discrete actions\n",
        "\n",
        "        epsilon = 1 # Initial epsilon\n",
        "\n",
        "        memory = ReplayMemory(self.replay_memory_size)\n",
        "\n",
        "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
        "        policy_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "        target_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "\n",
        "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
        "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "        # Policy network optimizer. \"Adam\" optimizer.\n",
        "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
        "\n",
        "        # Learning rate scheduler - learning rate decay\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=self.lr_step_size, gamma=self.lr_decay_gamma)\n",
        "\n",
        "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
        "        rewards_per_episode = []\n",
        "\n",
        "        # List to keep track of epsilon decay\n",
        "        epsilon_history = []\n",
        "\n",
        "        # Track number of steps taken. Used for syncing policy => target network.\n",
        "        step_count=0\n",
        "        best_rewards=-200 # Adjusted initial best_rewards for continuous env\n",
        "        goal_reached=False\n",
        "\n",
        "        for i in range(episodes):\n",
        "\n",
        "            self.max_position = env.observation_space.low[0]\n",
        "            state = env.reset()[0]  # Initialize to state 0\n",
        "            terminated = False      # True when agent reached goal\n",
        "            truncated = False\n",
        "\n",
        "            rewards = 0\n",
        "\n",
        "            # Agent navigates map until it reaches goal (terminated), or is truncated.\n",
        "            while(not terminated and not truncated):\n",
        "\n",
        "                # Select action based on epsilon-greedy\n",
        "                if random.random() < epsilon:\n",
        "                    # select random action (index for discrete actions) uniformly\n",
        "                    action_index = random.randrange(self.num_discrete_actions)\n",
        "\n",
        "                else:\n",
        "                    # select best action (index for discrete actions)\n",
        "                    with torch.no_grad():\n",
        "                        # Use the continuous state as input and get the index of the best discrete action\n",
        "                        action_index = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
        "\n",
        "\n",
        "                # Map the discrete action index to the continuous action value\n",
        "                action = self.discrete_actions[action_index]\n",
        "\n",
        "                # Execute action - MountainCarContinuous expects a single value action in a list\n",
        "                new_state,reward,terminated,truncated,_ = env.step([action])\n",
        "\n",
        "                # Add a small negative reward at each timestep to discourage staying in the valley\n",
        "                reward -= 1\n",
        "\n",
        "                # Add positive reward for reaching new maximum position\n",
        "                if new_state[0] > self.max_position:\n",
        "                    reward += 10.0 # Add positive reward for progress\n",
        "                    self.max_position = new_state[0] # Update max position\n",
        "\n",
        "                # Accumulate reward\n",
        "                rewards += reward\n",
        "\n",
        "                # Save experience into memory\n",
        "                memory.append((state, action_index, new_state, reward, terminated)) # Store action_index, not continuous action value\n",
        "\n",
        "                # Move to the next state\n",
        "                state = new_state\n",
        "\n",
        "                # Increment step counter\n",
        "                step_count+=1\n",
        "\n",
        "            # Keep track of the rewards collected per episode.\n",
        "            rewards_per_episode.append(rewards)\n",
        "\n",
        "            # Log reward per episode to wandb\n",
        "            wandb.log({\"reward_per_episode\": rewards}, step=i)\n",
        "\n",
        "\n",
        "            # Check if goal was reached\n",
        "            if(terminated):\n",
        "                goal_reached = True\n",
        "\n",
        "            # Graph training progress\n",
        "            #if(i!=0 and i%1000==0):\n",
        "                #print(f'Episode {i} Epsilon {epsilon}')\n",
        "\n",
        "                #self.plot_progress(rewards_per_episode, epsilon_history)\n",
        "                #torch.save(policy_dqn.state_dict(), f\"mountaincar_autosave_dql_{i}.pt\")\n",
        "\n",
        "\n",
        "            if rewards>best_rewards:\n",
        "                best_rewards = rewards\n",
        "                print(f'Best rewards so far: {best_rewards}')\n",
        "                # Save policy\n",
        "                torch.save(policy_dqn.state_dict(), f\"mountaincar_dql_{i}.pt\")\n",
        "\n",
        "\n",
        "            # Check if enough experience has been collected AND goal was reached\n",
        "            if len(memory)>self.mini_batch_size and goal_reached:\n",
        "                mini_batch = memory.sample(self.mini_batch_size) # Use mini_batch_size for sampling\n",
        "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
        "\n",
        "                if(self.linearDecay):\n",
        "                  epsilon = max(epsilon - 1/episodes, self.epsilon_min)\n",
        "                elif(self.hyperbolicDecay):\n",
        "                  epsilon = max(self.epsilon_decay_c1 / (self.epsilon_decay_c2 + i), self.epsilon_min)\n",
        "                elif(self.exponentialDecay):\n",
        "                  epsilon = self.epsilon_min + (1 - self.epsilon_min) * math.exp(-self.epsilon_decay_rate * i)\n",
        "\n",
        "                epsilon_history.append(epsilon)\n",
        "                # Log epsilon to wandb\n",
        "                wandb.log({\"epsilon\": epsilon}, step=i)\n",
        "\n",
        "                # Step the learning rate scheduler - learning rate decay\n",
        "                self.scheduler.step()\n",
        "                # Log current learning rate to wandb\n",
        "                wandb.log({\"learning_rate\": self.optimizer.param_groups[0]['lr']}, step=i)\n",
        "\n",
        "\n",
        "                # Copy policy network to target network after a certain number of steps\n",
        "                if step_count > self.network_sync_rate:\n",
        "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "                    step_count=0\n",
        "\n",
        "        # Save the final model\n",
        "        #torch.save(policy_dqn.state_dict(), \"mountaincar_dql_final.pt\")\n",
        "        #print(\"Final model saved as mountaincar_dql_final.pt\")\n",
        "\n",
        "        # Close environment\n",
        "        env.close()\n",
        "    #def plot_progress(self, rewards_per_episode, epsilon_history):\n",
        "        # Create new graph\n",
        "        #plt.figure(1)\n",
        "\n",
        "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
        "        # rewards_curve = np.zeros(len(rewards_per_episode))\n",
        "        # for x in range(len(rewards_per_episode)):\n",
        "            # rewards_curve[x] = np.min(rewards_per_episode[max(0, x-10):(x+1)])\n",
        "        #plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
        "        # plt.plot(sum_rewards)\n",
        "        #plt.plot(rewards_per_episode)\n",
        "\n",
        "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
        "        #plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
        "        #plt.plot(epsilon_history)\n",
        "\n",
        "        # Save plots\n",
        "        #plt.savefig('mountaincar_dql.png')\n",
        "    # Optimize policy network\n",
        "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
        "\n",
        "        current_q_list = []\n",
        "        target_q_list = []\n",
        "\n",
        "        for state, action_index, new_state, reward, terminated in mini_batch: # Use action_index\n",
        "\n",
        "            if terminated:\n",
        "                # Agent receive reward of 100 for reaching goal.\n",
        "                # When in a terminated state, target q value should be set to the reward.\n",
        "                target = torch.FloatTensor([reward])\n",
        "            else:\n",
        "                # Calculate target q value\n",
        "                with torch.no_grad():\n",
        "                    # Use the continuous state as input\n",
        "                    target = torch.FloatTensor(\n",
        "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state)).max()\n",
        "                    )\n",
        "\n",
        "            # Get the current set of Q values\n",
        "            # Use the continuous state as input\n",
        "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
        "            current_q_list.append(current_q)\n",
        "\n",
        "            # Get the target set of Q values\n",
        "            # Use the continuous state as input\n",
        "            target_q = target_dqn(self.state_to_dqn_input(state))\n",
        "            # Adjust the specific action (index) to the target that was just calculated\n",
        "            target_q[action_index] = target\n",
        "            target_q_list.append(target_q)\n",
        "\n",
        "        # Compute loss for the whole minibatch\n",
        "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    '''\n",
        "    Converts a state (position, velocity) to tensor representation for continuous observation space.\n",
        "    Example:\n",
        "    Input = (0.3, -0.03)\n",
        "    Return = tensor([0.3, -0.03])\n",
        "    '''\n",
        "    def state_to_dqn_input(self, state)->torch.Tensor:\n",
        "        # The state is already a NumPy array [position, velocity]\n",
        "        # Convert it directly to a PyTorch FloatTensor\n",
        "        return torch.FloatTensor(state)\n",
        "\n",
        "    # Run the environment with the learned policy\n",
        "    def test(self, episodes, model_filepath):\n",
        "\n",
        "        env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
        "        env = gym.wrappers.RecordVideo(env, video_folder='mountaincar_test_video', episode_trigger=lambda x: True) # Record every episode\n",
        "\n",
        "        # Set the seed for the environment\n",
        "        if self.seed is not None:\n",
        "            env.reset(seed=self.seed)\n",
        "\n",
        "        # Get continuous action space bounds\n",
        "        min_action = env.action_space.low[0]\n",
        "        max_action = env.action_space.high[0]\n",
        "\n",
        "        # Create discrete actions using linspace\n",
        "        self.discrete_actions = np.linspace(min_action, max_action, self.num_discrete_actions)\n",
        "\n",
        "\n",
        "        num_states = env.observation_space.shape[0]\n",
        "        num_actions = self.num_discrete_actions # Use the number of discrete actions\n",
        "\n",
        "\n",
        "\n",
        "        # Load learned policy\n",
        "        policy_dqn = DQN(input_dim=num_states, num_actions=num_actions)\n",
        "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
        "        policy_dqn.eval()    # switch model to evaluation mode\n",
        "\n",
        "        total_test_rewards = 0\n",
        "        test_rewards_list = []\n",
        "        for i in range(episodes):\n",
        "            state = env.reset()[0]  # Initialize to state 0\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "            rewards = 0\n",
        "\n",
        "            while(not terminated and not truncated):\n",
        "                # Select best action (index)\n",
        "                with torch.no_grad():\n",
        "                    # Use the continuous state as input and get the index of the best discrete action\n",
        "                    action_index = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
        "\n",
        "                # Map the discrete action index to the continuous action value\n",
        "                action = self.discrete_actions[action_index]\n",
        "\n",
        "                # Execute action - MountainCarContinuous expects a single value action in a list\n",
        "                state,reward,terminated,truncated,_ = env.step([action])\n",
        "                rewards += reward\n",
        "\n",
        "            total_test_rewards += rewards\n",
        "            test_rewards_list.append(rewards)\n",
        "\n",
        "            # Check if the goal was reached (terminated without truncation)\n",
        "            # MountainCarContinuous-v0 terminates when the flag is reached\n",
        "            if terminated:\n",
        "                print(f\"Episode {i+1}: Goal Reached! Reward: {rewards}\")\n",
        "            elif truncated:\n",
        "                print(f\"Episode {i+1}: Episode truncated (did not reach goal). Reward: {rewards}\")\n",
        "            else: # This case should not happen in MountainCarContinuous if not truncated\n",
        "                 print(f\"Episode {i+1}: Episode terminated unexpectedly. Reward: {rewards}\")\n",
        "\n",
        "        # Calculate and Log average test reward to wandb\n",
        "        if episodes > 0:\n",
        "            avg_test_reward = total_test_rewards / episodes\n",
        "            wandb.log({\"average_test_reward\": avg_test_reward})\n",
        "            print(f\"Average test reward over {episodes} episodes: {avg_test_reward}\")\n",
        "        else:\n",
        "            print(\"No test episodes run.\")\n",
        "\n",
        "        # Log test videos to wandb\n",
        "        # Assuming videos are saved in 'mountaincar_test_video' directory\n",
        "        # Wandb can log video files directly.\n",
        "        # We need to find the video files generated during this test run.\n",
        "        # The RecordVideo wrapper names videos based on the episode index.\n",
        "        video_files = glob.glob('mountaincar_test_video/rl-video-episode-*.mp4')\n",
        "        if video_files:\n",
        "            print(f\"Logging {len(video_files)} test videos to wandb.\")\n",
        "            for video_file in video_files:\n",
        "                wandb.log({\"test_video\": wandb.Video(video_file)})\n",
        "        else:\n",
        "            print(\"No test videos found to log.\")\n",
        "\n",
        "\n",
        "        env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "uO1dLIixc-VK",
        "outputId": "290684ec-00aa-474b-e03c-3ffb490fbe33"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_test_modified_reward</td><td></td></tr><tr><td>average_test_reward</td><td></td></tr><tr><td>epsilon</td><td></td></tr><tr><td>learning_rate</td><td></td></tr><tr><td>reward_per_episode</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_test_modified_reward</td><td>-17.0479</td></tr><tr><td>average_test_reward</td><td>94.0021</td></tr><tr><td>epsilon</td><td>0.13045</td></tr><tr><td>learning_rate</td><td>0.00387</td></tr><tr><td>reward_per_episode</td><td>-1027.17828</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">MountainCar_DQL_Run_101_Actions</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/7i9ve9fw' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/7i9ve9fw</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a><br>Synced 5 W&B file(s), 21 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250912_114610-7i9ve9fw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "creating run (3.5s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250912_121413-r74wq04w</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/r74wq04w' target=\"_blank\">MountainCar_DQL_Run_1001_Actions</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/r74wq04w' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/r74wq04w</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matteo-piras-universit-di-firenze/MountainCar%20DQL/runs/r74wq04w?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f57e86f1d60>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "num_discrete_actions=1001\n",
        "learning_rate_a=0.01\n",
        "discount_factor_g=0.998\n",
        "seed=2025\n",
        "network_sync_rate=100\n",
        "replay_memory_size=100000\n",
        "mini_batch_size=64\n",
        "lr_decay_gamma=0.95\n",
        "lr_step_size=500\n",
        "#epsilon_decay_c1=2000\n",
        "#epsilon_decay_c2=2000\n",
        "epsilon_decay_rate=0.00015\n",
        "epsilon_min=0.01\n",
        "# Define hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate_a\": learning_rate_a,\n",
        "    \"discount_factor_g\": discount_factor_g,\n",
        "    \"network_sync_rate\": network_sync_rate,\n",
        "    \"replay_memory_size\": replay_memory_size,\n",
        "    \"mini_batch_size\": mini_batch_size,\n",
        "    \"num_discrete_actions\": num_discrete_actions,\n",
        "    \"seed\": seed,\n",
        "    \"lr_decay_gamma\": lr_decay_gamma,\n",
        "    \"lr_step_size\": lr_step_size,\n",
        "    #\"epsilon_decay_c1\": epsilon_decay_c1,\n",
        "    #\"epsilon_decay_c2\": epsilon_decay_c2,\n",
        "    \"epsilon_decay_rate\": epsilon_decay_rate,\n",
        "    \"epsilon_min\": epsilon_min\n",
        "\n",
        "}\n",
        "\n",
        "wandb.init(project=\"MountainCar DQL\", name=\"MountainCar_DQL_Run_1001_Actions\", config=hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "19e1d454",
        "outputId": "44ca7cbb-0e56-4548-a1dc-950ddcf7585e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best rewards so far: 26.60649172903551\n",
            "Best rewards so far: 158.26990197470775\n",
            "Best rewards so far: 179.49498937504387\n",
            "Best rewards so far: 238.2503736360872\n",
            "Best rewards so far: 259.57271020660414\n",
            "Best rewards so far: 277.86612101241656\n",
            "Best rewards so far: 356.6220806970288\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3308725789.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                              )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmountaincar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3531091011.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes, render)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini_batch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgoal_reached\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use mini_batch_size for sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinearDecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3531091011.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, mini_batch, policy_dqn, target_dqn)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "mountaincar = MountainCarDQL(num_discrete_actions=num_discrete_actions,\n",
        "                             learning_rate_a=learning_rate_a,\n",
        "                             discount_factor_g=discount_factor_g,\n",
        "                             seed=seed,\n",
        "                             lr_step_size=lr_step_size,\n",
        "                             epsilon_decay_rate=epsilon_decay_rate,\n",
        "                             epsilon_min=epsilon_min\n",
        "                             )\n",
        "\n",
        "mountaincar.train(10000, False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}